<html>

<head>
  <meta content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org" name="generator" />
</head>
<!--<link href="i_mark_bold.png" type="image/png" rel="icon" />-->
<script src="hidebib.js" type="text/javascript"></script>
<title>Matthew Chang</title>
<link href="https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic" type="text/css" rel="stylesheet" />
<link href="style.css" type="text/css" rel="stylesheet" />

<body>
  <table cellpadding="10" width="1000" align="center" border="0">
    <tr>
      <td halign="center">
        <p align="center">
          <font size="6">Matthew Chang</font>
        </p>
      </td>
    </tr>
    <tr>
      <td>
        <table cellpadding="10" width="100%" align="center" border="0">
          <tr>
            <td width="50%" valign="top">
              <p>I am an PhD student at the University of Illinois
                Urbana-Champaign, being advised by Saurabh Gupta. I did my
                undergrad and Master's degree at MIT. I study machine vision,
                robotics and reinforcement learning. I am interested in
                developing agents which can effectively interact with the world
                around them in a safe and intelligent manner. I am focusing on
                classical robotic methods combined with reinforcement learning,
                and methods for utilizing human demonstrations, or other
                offline data, for enhancing robotic performance.
            </td>
            <td width="15%" valign="top">
              <img src="images/headshot.jpg" style="border-style: none" width="100%" />
            </td>
          </tr>
        </table>
      </td>
    </tr>
    <tr>
      <!--<td>-->
      <!--<heading>Research</heading>-->
      <!--<br />-->
      <!--<p>I work on computer vision, robotics and machine learning. I am interested in building agents that can intelligently interact with the physical world around them. I am currently focusing on two aspects: a) choice and design of representations that enable such interaction, and b) what and how to learn from active interaction. Some example problems that we are tackling include: building <a href="https://arxiv.org/pdf/1702.03920.pdf">spatio-semantic</a> and <a href="http://www.cs.cmu.edu/~dchaplot/projects/neural-topological-slam.html">topological</a> representations for visual navigation, <a href="https://sites.google.com/view/iclr2020-synergistic">skill discovery</a>, learning <a href="https://arxiv.org/pdf/2007.04515.pdf">for</a> and <a href="https://matthewchang.github.io/value-learning-from-videos/">from</a> videos, and <a href="https://www.cs.cmu.edu/~dchaplot/projects/SemanticCuriosity.html">active visual leaning</a>.</p>-->
      <!--<p>My research is funded through National Science Foundation, DARPA Machine Common Sense Program, NASA, USDA/NSF AIFARMS National AI Institute, and Amazon.</p>-->
      <!--</td>-->
    </tr>
    <tr>
    </tr>
    <tr>
      <td>
        <heading>Recent Talks</heading>
        <br />
        <ul>
          <li><a href="https://youtu.be/QlOK1-hXFGc?t=12240">Semantic Visual Navigation by Watching YouTube Videos</a> at <a href="https://eyewear-computing.org/EPIC_ECCV20/">International Workshop on Egocentric Perception, Interaction and Computing (EPIC) 2020</a>.</li>
        </ul>

        <ul>
          <li><a href="https://www.youtube.com/watch?v=-t4-DtZoJeM&t=1248s">Semantic Visual Navigation by Watching YouTube Videos</a> at <a href="https://askforalfred.com/EVAL/">
              Embodied Vision, Actions & Language Workshop at ECCV 2020</a>.</li>
        </ul>
      </td>
    </tr>
    <tr>
      <!--<td>-->
      <!--<heading>Teaching</heading>-->
      <!--<table cellpadding="10" width="100%" align="center" border="0">-->
      <!--<tr>-->
      <!--<td width="80%" valign="top"><strong>ECE 598 SG: Special Topics in Learning-based Robotics</strong>: <a href="https://s-gupta.github.io/ece598sg/">Fall 2019</a>, <a href="http://saurabhg.web.illinois.edu/teaching/ece598sg/fa2020/">Fall 2020</a>, <a href="http://saurabhg.web.illinois.edu/teaching/ece598sg/fa2021/">Fall 2021</a></td>-->
      <!--</tr>-->
      <!--<tr>-->
      <!--<td width="80%" valign="top"><strong>ECE 549 / CS 543: Computer Vision</strong>: <a href="http://saurabhg.web.illinois.edu/teaching/ece549/sp2020/">Spring 2020</a>, <a href="http://saurabhg.web.illinois.edu/teaching/ece549/sp2021/">Spring 2021</a></td>-->
      <!--</tr>-->
      <!--</table>-->
      <!--</td>-->
    </tr>
    <tr>
      <td>
        <heading>Publications</heading>
        <br />
        <br />
        <subheading>2022</subheading>
        <table cellpadding="10" width="100%" align="center" border="0">
        <tr>
            <td width="20%" valign="top">
              <img src="images/awda_thumb.png" style="border-style: none" width="100%" />
            </td>
            <td width="80%" valign="top">
              <papertitle><a href="https://matthewchang.github.io/awda_site/">One-shot Visual Imitation via Attributed Waypoints and Demonstration Augmentation</a></papertitle><br /><b>Matthew Chang</b>, Saurabh Gupta
<br /> In Submission ICRA 2023
<br /> Deep Reinforcement Learning Workshop NeurIPS 2022<br />

              <!--details-->
              <div class="paper" id="chang2022latent"><a href="javascript:toggleblock('chang2022one_abs')">abstract</a> / 
                  <!--<a href="javascript:toggleblock('chang2022one_bib')">bibtex</a> / -->
                  <a href="https://matthewchang.github.io/awda_site/">webpage</a> / 
                  <a href="https://openreview.net/pdf?id=0sasjKzmzk">paper</a> / 
                  <a href="https://youtu.be/iJnwT4tT6bg">video</a>
                <p align="justify"><i id="chang2022one_abs">
In this paper, we analyze the behavior of existing techniques and design new solutions for the problem of one-shot visual imitation. In this setting, an agent must solve a novel instance of a novel task given just a single visual demonstration. Our analysis reveals that current methods fall short because of three errors: the DAgger problem arising from purely offline training, last centimeter errors in interacting with objects, and mis-fitting to the task context rather than to the actual task.  This motivates the design of our modular approach where we a) separate out task inference (what to do) from task execution (how to do it), and b) develop data augmentation and generation techniques to mitigate mis-fitting.  The former allows us to leverage hand-crafted motor primitives for task execution which side-steps the DAgger problem and last centimeter errors, while the latter gets the model to focus on the task rather than the task context. Our model gets 100% and 48% success rates on two recent benchmarks, improving upon the current state-of-the-art by absolute 90% and 20% respectively. 
</i></p>
                <bibtext xml:space="preserve" id="chang2022one_bib">@inproceedings{chang2022learning,<br/>
                author = "Chang, Matthew and Gupta, Arjun and Gupta, Saurabh",<br/>
                title = "Learning Value Functions from Undirected State-only Experience",<br/>
                booktitle = "International Conference on Learning Representations",<br/>
                year = "2022"<br/>
}</bibtext>
              </div>
              <script language="JavaScript">
                hideblock('chang2022one_bib');
                hideblock('chang2022one_abs');
              </script>
              <!--end details-->
            </td>
          </tr>
        </table>
        <subheading>2021</subheading>
  <table cellpadding="10" width="100%" align="center" border="0">
          <tr>
            <td width="20%" valign="top">
              <img src="images/laqthumb.png" style="border-style: none" width="100%" />
            </td>
            <td width="80%" valign="top">
              <papertitle><a href="https://matthewchang.github.io/latent_action_qlearning_site/">Learning Value Functions from Undirected State-only Experience</a></papertitle><br /><b>Matthew Chang</b>, Arjun Gupta, Saurabh Gupta
<br />International Conference on Learning Representations (ICLR) 2022<br />
Embodied AI Workshop CVPR 2022<br/>
              Workshop on Offline Reinforcement Learning NeurIPS 2021 <br /> Deep Reinforcement Learning Workshop NeurIPS 2021<br />

              <!--details-->
              <div class="paper" id="chang2022latent"><a href="javascript:toggleblock('chang2022latent_abs')">abstract</a> / <a href="javascript:toggleblock('chang2022latent_bib')">bibtex</a> / <a href="https://matthewchang.github.io/latent_action_qlearning_site/">webpage</a> / <a href="https://arxiv.org/abs/2204.12458">arxiv link</a> / <a href="https://www.youtube.com/watch?v=QIRHDla_oGM;">video</a>
                <p align="justify"><i id="chang2022latent_abs">This paper tackles the problem of learning value functions from undirected state-only experience (state transitions without action labels i.e. (s,s',r) tuples). We first theoretically characterize the applicability of Q-learning in this setting. We show that tabular Q-learning in discrete Markov decision processes (MDPs) learns the same value function under any arbitrary refinement of the action space. This theoretical result motivates the design of Latent Action Q-learning or LAQ, an offline RL method that can learn effective value functions from state-only experience. Latent Action Q-learning (LAQ) learns value functions using Q-learning on discrete latent actions obtained through a latent-variable future prediction model. We show that LAQ can recover value functions that have high correlation with value functions learned using ground truth actions. Value functions learned using LAQ lead to sample efficient acquisition of goal-directed behavior, can be used with domain-specific low-level controllers, and facilitate transfer across embodiments. Our experiments in 5 environments ranging from 2D grid world to 3D visual navigation in realistic environments demonstrate the benefits of LAQ over simpler alternatives, imitation learning oracles, and competing methods. </i></p>
                <bibtext xml:space="preserve" id="chang2022latent_bib">@inproceedings{chang2022learning,<br/>
                author = "Chang, Matthew and Gupta, Arjun and Gupta, Saurabh",<br/>
                title = "Learning Value Functions from Undirected State-only Experience",<br/>
                booktitle = "International Conference on Learning Representations",<br/>
                year = "2022"<br/>
}</bibtext>
              </div>
              <script language="JavaScript">
                hideblock('chang2022latent_bib');
                hideblock('chang2022latent_abs');
              </script>
              <!--end details-->
            </td>
          </tr>
          <br />
      </td>
    </tr>
  </table>
  <subheading>2020</subheading>
  <table cellpadding="10" width="100%" align="center" border="0">
    <tr>
      <td width="20%" valign="top">
        <img src="images/chang2020semantic.jpg" style="border-style: none" width="100%" />
      </td>
      <td width="80%" valign="top">
        <papertitle><a href="https://arxiv.org/pdf/2006.10034.pdf">Semantic Visual Navigation by Watching YouTube Videos</a></papertitle><br /><b>Matthew Chang</b>, Arjun Gupta, Saurabh Gupta<br />Neural Information Processing Systems (NeurIPS) 2020<br />
        <div class="paper" id="chang2020semantic"><a href="javascript:toggleblock('chang2020semantic_abs')">abstract</a> / <a href="javascript:toggleblock('chang2020semantic_bib')">bibtex</a> / <a href="https://matthewchang.github.io/value-learning-from-videos/">webpage</a> / <a href="https://arxiv.org/abs/2006.10034">arxiv link</a> / <a href="https://www.youtube.com/watch?v=o8j4Ov_oF7A&amp;">video</a>
          <p align="justify"><i id="chang2020semantic_abs">Semantic cues and statistical regularities in real-world environment layouts can improve efficiency for navigation in novel environments. This paper learns and leverages such semantic cues for navigating to objects of interest in novel environments, by simply watching YouTube videos. This is challenging because YouTube videos do not come with labels for actions or goals, and may not even showcase optimal behavior. Our method tackles these challenges through the use of Q-learning on pseudo-labeled transition quadruples (image, action, next image, reward). We show that such off-policy Q-learning from passive data is able to learn meaningful semantic cues for navigation. These cues, when used in a hierarchical navigation policy, lead to improved efficiency at the ObjectGoal task in visually realistic simulations. We observe a relative improvement of 15-83% over end-to-end RL, behavior cloning, and classical methods, while using minimal direct interaction.</i></p>
          <bibtext xml:space="preserve" id="chang2020semantic_bib">@inproceedings{chang2020semantic,<br /> author = "Chang, Matthew and Gupta, Arjun and Gupta, Saurabh",<br /> title = "Semantic Visual Navigation by Watching YouTube Videos",<br /> booktitle = "Advances in Neural Information Processing Systems",<br /> year = "2020"<br />}<br /></bibtext>
        </div>
        <script language="JavaScript">
          hideblock('chang2020semantic_bib');
          hideblock('chang2020semantic_abs');
        </script>
      </td>
    </tr>
    <br />
    </td>
    </tr>
  </table>
</body>

</html>
